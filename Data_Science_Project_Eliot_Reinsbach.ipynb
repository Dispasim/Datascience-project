{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SlZy-Z0pGruF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d80d321-1940-4568-ca8a-0ac4d8ab26a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.11/dist-packages (0.38.0)\n",
            "Requirement already satisfied: numpy<2,>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (1.26.4)\n",
            "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.3.1)\n",
            "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.4.2)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.10.2.post1)\n",
            "Requirement already satisfied: scipy<1.13,>=1.4 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (1.12.0)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.13.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install audiomentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This notebook have been made on google colab. In a local environment, other library might have to be installed\n",
        "import cv2\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
        "import random\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "r4r_qjkTG5n8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms an audio file into a mel spectrogram\n",
        "def generate_mel_spectrogram(audio, sr):\n",
        "\n",
        "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels= 128, fmax= 8000,n_fft = 2048 , hop_length = 512 )\n",
        "\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    return S_dB\n",
        "\n",
        "\n",
        "#resize a spectrogram\n",
        "def resize_melspectrogram(melspectrogram):\n",
        "    resized_mel = resize(\n",
        "        melspectrogram,\n",
        "        (128, 128),\n",
        "        mode='constant',\n",
        "        anti_aliasing=True\n",
        "    )\n",
        "    return resized_mel"
      ],
      "metadata": {
        "id": "Q984M7T8G8St"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"development.csv\")#path to dataset\n",
        "df[\"tempo\"] = df[\"tempo\"].apply(ast.literal_eval)\n",
        "df[\"tempo\"] = df[\"tempo\"].apply(lambda x: x[0])\n",
        "df = pd.get_dummies(df, columns=['gender'], prefix='gender',dtype='int')\n"
      ],
      "metadata": {
        "id": "EywFuqgEp34E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import tha data"
      ],
      "metadata": {
        "id": "PdOnNut5JoBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "labels = []\n",
        "data = []\n",
        "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Traitement des lignes\", unit=\"ligne\"):\n",
        "\n",
        "\n",
        "  file_path = f\"{row['path']}\"#path to audios\n",
        "  age = row['age']\n",
        "  audio, sr = librosa.load(file_path)\n",
        "  mel_spec = generate_mel_spectrogram(audio,sr)\n",
        "  mel_spec = resize_melspectrogram(mel_spec)\n",
        "  #print(f\"Forme avant ajout de la dimension de couleur: {mel_spec.shape}\")\n",
        "  # Convertir en tableau d'images\n",
        "  mel_spec = np.expand_dims(mel_spec, axis=-1)  # Ajouter la dimension de couleur (1 canal)\n",
        "  #print(f\"Forme après ajout de la dimension de couleur: {mel_spec.shape}\")\n",
        "  mel_spec = img_to_array(mel_spec)  # Convertir en format utilisable par Keras\n",
        "  images.append(mel_spec)\n",
        "  labels.append(age)  # Assigner l'étiquette de la classe\n",
        "  vector = row[[\n",
        "    #'sampling_rate',\n",
        "    'gender_male',\n",
        "    'gender_female',\n",
        "    #'ethnicity',\n",
        "    'mean_pitch',\n",
        "    'max_pitch',\n",
        "    'min_pitch',\n",
        "    'jitter',\n",
        "    'shimmer',\n",
        "    'energy',\n",
        "    'zcr_mean',\n",
        "    'spectral_centroid_mean',\n",
        "    'tempo',\n",
        "    'hnr',\n",
        "    'num_words',\n",
        "    'num_characters',\n",
        "    'num_pauses',\n",
        "    'silence_duration',\n",
        "    #'path'\n",
        "    ]]\n",
        "\n",
        "  data.append(vector.to_numpy())"
      ],
      "metadata": {
        "id": "MmDjYNYcJUkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "standization"
      ],
      "metadata": {
        "id": "Tcq5SGOGJnSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_images = np.array(images)\n",
        "X_data = np.array(data)\n",
        "y = np.array(labels)\n",
        "\n",
        "X_data = [(X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) for X in X_data]\n",
        "X_images = [(X - X.min()) / (X.max() - X.min()) for X in X_images]\n",
        "\n",
        "X_images_train, X_images_test, X_data_train, X_data_test, y_train, y_test = train_test_split(\n",
        "    X_images, X_data, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_images_test = np.array(X_images_test, dtype=np.float32)\n",
        "X_data_test = np.array(X_data_test, dtype=np.float32)\n",
        "\n",
        "X_images_train = np.array(X_images_train, dtype=np.float32)\n",
        "X_data_train = np.array(X_data_train, dtype=np.float32)"
      ],
      "metadata": {
        "id": "ggDgeJlPLst6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logarithm transformation"
      ],
      "metadata": {
        "id": "7Dx1vYx5L3yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I put it un comment because it makes the results less readable\n",
        "\"\"\"y_train_log = np.log1p(y_train)\n",
        "y_test_log = np.log1p(y_test)\"\"\""
      ],
      "metadata": {
        "id": "e-iwIuGwL7Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "rljIuWIzMsG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate,Multiply\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "input_img = Input(shape=(128, 128, 1))\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "#input data\n",
        "input_data = Input(shape=(X_data_train.shape[1],))\n",
        "data_transformed  = Dense(64, activation='relu')(input_data)\n",
        "data_transformed = Dense(32, activation='relu')(data_transformed )\n",
        "merged = concatenate([x, data_transformed ])\n",
        "\n",
        "merged = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
        "merged = Dense(32, activation='relu')(merged)\n",
        "output = Dense(1, activation='linear')(merged)  #linear activation for regression\n",
        "\n",
        "model_final = Model(inputs=[input_img, input_data], outputs=output)\n",
        "model_final.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model_final.summary()\n",
        "\n",
        "\n",
        "model_final.fit([X_images_train, X_data_train], y_train, epochs=25, batch_size=32,validation_data = ([X_images_test, X_data_test], y_test))"
      ],
      "metadata": {
        "id": "WLwgPIyKMtb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation"
      ],
      "metadata": {
        "id": "KwSFZK0WNNyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss_eval, test_rmse_eval = model_final.evaluate([X_images_test, X_data_test], y_test)\n",
        "print(f'Loss: {test_loss_eval}, RMSE: {test_rmse_eval}')"
      ],
      "metadata": {
        "id": "7Za3VK18NNVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}